import torch
from torch.utils.data import Dataset
from transformers import (
    VoxtralForConditionalGeneration, 
    AutoProcessor,
    Trainer,
    TrainingArguments
)
import json
from pathlib import Path
from typing import Dict, List, Optional
from torch.utils.tensorboard import SummaryWriter
from transformers import DataCollatorWithPadding


class VoxtralDataCollator:
    """Voxtral ëª¨ë¸ì„ ìœ„í•œ ì»¤ìŠ¤í…€ ë°ì´í„° ì½œë ˆì´í„°"""
    
    def __init__(self, processor):
        self.processor = processor
    
    def __call__(self, features):
        batch = {}
        
        # input_idsì™€ attention_mask ì²˜ë¦¬
        if 'input_ids' in features[0]:
            input_ids = [f['input_ids'] for f in features]
            attention_mask = [f['attention_mask'] for f in features]
            
            # íŒ¨ë”©ìœ¼ë¡œ í¬ê¸° ë§ì¶”ê¸°
            max_length = max(len(ids) for ids in input_ids)
            
            padded_input_ids = []
            padded_attention_mask = []
            
            for ids, mask in zip(input_ids, attention_mask):
                # íŒ¨ë”© í¬ê¸° ê³„ì‚°
                padding_length = max_length - len(ids)
                
                # input_ids íŒ¨ë”©
                padded_ids = torch.cat([ids, torch.zeros(padding_length, dtype=ids.dtype)])
                padded_input_ids.append(padded_ids)
                
                # attention_mask íŒ¨ë”©
                padded_mask = torch.cat([mask, torch.zeros(padding_length, dtype=mask.dtype)])
                padded_attention_mask.append(padded_mask)
            
            batch['input_ids'] = torch.stack(padded_input_ids)
            batch['attention_mask'] = torch.stack(padded_attention_mask)
        
        # labels ì²˜ë¦¬
        if 'labels' in features[0]:
            labels = [f['labels'] for f in features]
            max_length = max(len(label) for label in labels)
            
            padded_labels = []
            for label in labels:
                padding_length = max_length - len(label)
                padded_label = torch.cat([label, torch.full((padding_length,), -100, dtype=label.dtype)])
                padded_labels.append(padded_label)
            
            batch['labels'] = torch.stack(padded_labels)
        
        # input_features ì²˜ë¦¬ (ì˜¤ë””ì˜¤ê°€ ìˆëŠ” ê²½ìš°)
        if 'input_features' in features[0]:
            input_features = [f['input_features'] for f in features if f.get('input_features') is not None]
            if input_features:
                # ëª¨ë“  input_featuresê°€ ë™ì¼í•œ í¬ê¸°ë¥¼ ê°€ì§€ë„ë¡ ë³´ì¥
                target_size = 3000
                processed_features = []
                
                for feature in input_features:
                    if len(feature.shape) == 2:
                        if feature.shape[1] > target_size:
                            feature = feature[:, :target_size]
                        elif feature.shape[1] < target_size:
                            padding_size = target_size - feature.shape[1]
                            feature = torch.nn.functional.pad(
                                feature, 
                                (0, padding_size), 
                                mode='constant', 
                                value=0
                            )
                    processed_features.append(feature)
                
                if processed_features:
                    batch['input_features'] = torch.stack(processed_features)
        
        # ë°°ì¹˜ í¬ê¸° í™•ì¸ ë° ë””ë²„ê¹…
        batch_sizes = {k: v.shape[0] if hasattr(v, 'shape') else len(v) for k, v in batch.items()}
        print(f"ğŸ” ë°°ì¹˜ í¬ê¸°: {batch_sizes}")
        
        return batch


class VoxtralDomainDataset(Dataset):
    """ë„ë©”ì¸ íŠ¹í™” ë°ì´í„°ì…‹ í´ë˜ìŠ¤"""
    
    def __init__(self, data_path: str, processor, model_id: str, max_length: int = 2048):
        self.processor = processor
        self.model_id = model_id  # model_id ì¶”ê°€
        self.max_length = max_length
        self.data = self._load_data(data_path)
    
    def _load_data(self, data_path: str) -> List[Dict]:
        """ë°ì´í„° ë¡œë“œ (JSON í˜•ì‹ ì˜ˆìƒ)"""
        with open(data_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        audio_path = item.get('audio_path', None)
        if audio_path is None:      
            conversation = [{
                "role": "user",
                "content": [
                    {"type": "text", "text": item['input']}
                ]
            }]
            
            inputs = self.processor.apply_chat_template(
                conversation, 
                return_tensors="pt",
                truncation=True,
                padding="max_length",
                max_length=self.max_length
            )
        else:
            # ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì‹œ ê³ ì •ëœ í¬ê¸°ë¡œ íŒ¨ë”©
            inputs = self.processor.apply_transcription_request(
                language="ko", 
                audio=audio_path, 
                model_id=self.model_id,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=self.max_length
            )
        
        target_text = item['output']
        target_encoding = self.processor.tokenizer(
            target_text,
            return_tensors="pt",
            truncation=True,
            padding="max_length",
            max_length=self.max_length
        )
        
        # input_features ì²˜ë¦¬ ê°œì„ 
        input_features = inputs.get('input_features')
        if input_features is not None:
            input_features = input_features.squeeze()
            # ì˜¤ë””ì˜¤ í”¼ì²˜ê°€ 2Dì¸ ê²½ìš° ì²˜ë¦¬
            if len(input_features.shape) == 2:
                # ê³ ì •ëœ í¬ê¸°ë¡œ íŒ¨ë”© ë˜ëŠ” ì˜ë¼ë‚´ê¸°
                target_size = 3000  # Qwen2Audioê°€ ìš”êµ¬í•˜ëŠ” mel í”¼ì²˜ ê¸¸ì´
                if input_features.shape[1] > target_size:
                    input_features = input_features[:, :target_size]
                elif input_features.shape[1] < target_size:
                    # íŒ¨ë”©ìœ¼ë¡œ í¬ê¸° ë§ì¶”ê¸°
                    padding_size = target_size - input_features.shape[1]
                    input_features = torch.nn.functional.pad(
                        input_features, 
                        (0, padding_size), 
                        mode='constant', 
                        value=0
                    )
        
        # ëª¨ë“  í…ì„œê°€ ë™ì¼í•œ í¬ê¸°ë¥¼ ê°€ì§€ë„ë¡ ë³´ì¥
        result = {
            'input_ids': inputs['input_ids'].squeeze(),
            'attention_mask': inputs['attention_mask'].squeeze(),
            'labels': target_encoding['input_ids'].squeeze()
        }
        
        # input_featuresê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€
        if input_features is not None:
            result['input_features'] = input_features
            
        # ë””ë²„ê¹…ì„ ìœ„í•œ ì •ë³´ ì¶”ê°€
        if idx < 3:  # ì²˜ìŒ 3ê°œ ìƒ˜í”Œë§Œ ì¶œë ¥
            print(f"ğŸ“Š ìƒ˜í”Œ {idx}: input_ids={result['input_ids'].shape}, labels={result['labels'].shape}")
            if 'input_features' in result:
                print(f"   input_features={result['input_features'].shape}")
            
        return result


class VoxtralDomainTrainer:
    """Voxtral ë„ë©”ì¸ ì ì‘ íŠ¸ë ˆì´ë„ˆ"""
    
    def __init__(self, model_id: str = "mistralai/Voxtral-Mini-3B-2507"):
        self.model_id = model_id
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.processor = AutoProcessor.from_pretrained(model_id)
        self.model = None
        self.writer = None  # í…ì„œë³´ë“œ SummaryWriter
    
    def setup_model(self, strategy: str = "full"):
        """íŒŒì¸íŠœë‹ ì „ëµì— ë”°ë¥¸ ëª¨ë¸ ì„¤ì •"""
        print(f"ëª¨ë¸ ì„¤ì • ì¤‘ - ì „ëµ: {strategy}")
        
        self.model = VoxtralForConditionalGeneration.from_pretrained(
            self.model_id,
            torch_dtype=torch.bfloat16 if self.device == "cuda" else torch.float32,
            device_map=self.device
        )
        
        if strategy == "frozen_audio":
            # ì˜¤ë””ì˜¤ ì¸ì½”ë” ë™ê²°
            for param in self.model.audio_tower.parameters():
                param.requires_grad = False
            print("âœ… ì˜¤ë””ì˜¤ ì¸ì½”ë” ë™ê²° ì™„ë£Œ")
            
        elif strategy == "frozen_lm":
            # ì–¸ì–´ ëª¨ë¸ ë™ê²°
            for param in self.model.language_model.parameters():
                param.requires_grad = False
            print("âœ… ì–¸ì–´ ëª¨ë¸ ë™ê²° ì™„ë£Œ")
            
        elif strategy == "projector_only":
            # í”„ë¡œì í„°ë§Œ í•™ìŠµ
            for param in self.model.parameters():
                param.requires_grad = False
            for param in self.model.multi_modal_projector.parameters():
                param.requires_grad = True
            print("âœ… í”„ë¡œì í„°ë§Œ í•™ìŠµ ì„¤ì • ì™„ë£Œ")
            
        elif strategy == "lora":
            # LoRA ì–´ëŒ‘í„° ì¶”ê°€ (ì–¸ì–´ ëª¨ë¸ì—ë§Œ)
            self._setup_lora()
            print("âœ… LoRA ì–´ëŒ‘í„° ì„¤ì • ì™„ë£Œ")
            
        elif strategy == "full":
            # ì „ì²´ ëª¨ë¸ íŒŒì¸íŠœë‹
            print("âœ… ì „ì²´ ëª¨ë¸ íŒŒì¸íŠœë‹ ì„¤ì • ì™„ë£Œ")
        
        return self.model
    
    def _setup_lora(self):
        """LoRA ì–´ëŒ‘í„° ì„¤ì •"""
        from peft import LoraConfig, get_peft_model, TaskType
        
        # LoRA ì„¤ì •
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=16,
            lora_alpha=32,
            lora_dropout=0.1,
            target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
        )
        
        # ì–¸ì–´ ëª¨ë¸ì—ë§Œ LoRA ì ìš©
        self.model.language_model = get_peft_model(self.model.language_model, lora_config)
    
    def train(self, 
              train_dataset: Dataset,
              val_dataset: Optional[Dataset] = None,
              output_dir: str = "./voxtral_finetuned",
              num_epochs: int = 3,
              batch_size: int = 1,
              learning_rate: float = 5e-5,
              gradient_accumulation_steps: int = 4):
        """ëª¨ë¸ í›ˆë ¨ ë° í…ì„œë³´ë“œ ë¡œê¹…"""
        
        # í…ì„œë³´ë“œ SummaryWriter ìƒì„±
        self.writer = SummaryWriter(log_dir=output_dir + "/runs")
        
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=num_epochs,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            gradient_accumulation_steps=gradient_accumulation_steps,
            learning_rate=learning_rate,
            weight_decay=0.01,
            logging_steps=10,
            save_steps=500,
            eval_steps=500 if val_dataset else None,
            eval_strategy="steps" if val_dataset else "no",
            save_strategy="steps",
            load_best_model_at_end=True if val_dataset else False,
            metric_for_best_model="eval_loss" if val_dataset else None,
            greater_is_better=False,
            warmup_steps=100,
            fp16=False,
            dataloader_pin_memory=False,
            remove_unused_columns=False,
            report_to=["tensorboard"],
            gradient_checkpointing=True, 
            max_grad_norm=1.0, 
            logging_first_step=True,
            save_total_limit=2,
        )
        
        # ì»¤ìŠ¤í…€ ë°ì´í„° ì½œë ˆì´í„° ì‚¬ìš©
        data_collator = VoxtralDataCollator(self.processor)
        
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            tokenizer=self.processor.tokenizer,
            data_collator=data_collator,
        )
        
        print("ğŸš€ í›ˆë ¨ ì‹œì‘...")
        trainer.train()
        
        # ëª¨ë¸ ì €ì¥
        trainer.save_model()
        self.processor.save_pretrained(output_dir)
        print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {output_dir}")
        print(f"ğŸ“ˆ í…ì„œë³´ë“œ ë¡œê·¸ ë””ë ‰í† ë¦¬: {output_dir}/runs")
        print("í…ì„œë³´ë“œ ì‹¤í–‰: tensorboard --logdir", output_dir + "/runs")
        
        self.writer.close()
        return trainer

strategy = "lora"  # íŒŒì¸íŠœë‹ ì „ëµ: full, frozen_audio, frozen_lm, projector_only, lora ì¤‘ ì„ íƒ
data_path = "train_data.json"  # í›ˆë ¨ ë°ì´í„° ê²½ë¡œ
val_path = 'validation_data.json'
output_dir = "./voxtral_domain_adapted"  # ì¶œë ¥ ë””ë ‰í† ë¦¬
model_id = "mistralai/Voxtral-Mini-3B-2507"  # ë² ì´ìŠ¤ ëª¨ë¸

# íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
trainer = VoxtralDomainTrainer(model_id)

# ëª¨ë¸ ì„¤ì •
model = trainer.setup_model(strategy)

# ë°ì´í„°ì…‹ ìƒì„±
train_dataset = VoxtralDomainDataset(data_path, trainer.processor, model_id)
val_dataset = VoxtralDomainDataset(val_path, trainer.processor, model_id)  # val_pathëŠ” ë³„ë„ ë³€ìˆ˜ë¡œ ì •ì˜ í•„ìš”

print(f"ğŸ“Š í›ˆë ¨ ë°ì´í„°: {len(train_dataset)}ê°œ")
print(f"ğŸ“Š í‰ê°€ ë°ì´í„°: {len(val_dataset)}ê°œ")
print(f"ğŸ¯ ì „ëµ: {strategy}")
print(f"ğŸ“ ì¶œë ¥: {output_dir}")

# í›ˆë ¨ ì‹¤í–‰ (ë°ì´í„°ì…‹ í¬ê¸° ì œí•œìœ¼ë¡œ í…ŒìŠ¤íŠ¸)
print("ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ë°ì´í„°ì…‹ í¬ê¸° ì œí•œ")
train_dataset_small = torch.utils.data.Subset(train_dataset, range(min(1000, len(train_dataset))))
val_dataset_small = torch.utils.data.Subset(val_dataset, range(min(100, len(val_dataset))))

trainer.train(
    train_dataset=train_dataset_small,
    val_dataset=val_dataset_small,
    output_dir=output_dir,
    num_epochs=1,  # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 1 ì—í¬í¬ë§Œ
)
