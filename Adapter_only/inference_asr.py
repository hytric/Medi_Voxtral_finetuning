# -*- coding: utf-8 -*-
import os
import json
import argparse
import unicodedata
import torch
from transformers import VoxtralForConditionalGeneration, AutoProcessor
from peft import PeftModel

def _norm(s: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ê·œí™”"""
    s = unicodedata.normalize("NFKC", s)
    return "".join(ch for ch in s if not ch.isspace())

def cer(ref: str, hyp: str) -> float:
    """Character Error Rate ê³„ì‚°"""
    r, h = _norm(ref), _norm(hyp)
    R, H = len(r), len(h)
    if R == 0:
        return 0.0 if H == 0 else 1.0
    
    dp = [[0]*(H+1) for _ in range(R+1)]
    for i in range(R+1): 
        dp[i][0] = i
    for j in range(H+1): 
        dp[0][j] = j
    
    for i in range(1, R+1):
        for j in range(1, H+1):
            cost = 0 if r[i-1] == h[j-1] else 1
            dp[i][j] = min(dp[i-1][j]+1, dp[i][j-1]+1, dp[i-1][j-1]+cost)
    
    return dp[R][H] / R

def wer(ref: str, hyp: str) -> float:
    """Word Error Rate ê³„ì‚°"""
    r = (ref or "").strip().split()
    h = (hyp or "").strip().split()
    R, H = len(r), len(h)
    if R == 0:
        return 0.0 if H == 0 else 1.0
    
    dp = [[0]*(H+1) for _ in range(R+1)]
    for i in range(R+1): 
        dp[i][0] = i
    for j in range(H+1): 
        dp[0][j] = j
    
    for i in range(1, R+1):
        for j in range(1, H+1):
            cost = 0 if r[i-1] == h[j-1] else 1
            dp[i][j] = min(dp[i-1][j]+1, dp[i][j-1]+1, dp[i-1][j-1]+cost)
    
    return dp[R][H] / R

@torch.no_grad()
def inference_single(model, processor, audio_path: str, model_id: str, language: str = "ko", max_new_tokens: int = 256):
    """ë‹¨ì¼ ì˜¤ë””ì˜¤ íŒŒì¼ì— ëŒ€í•œ ìŒì„± ì¸ì‹"""
    device = next(model.parameters()).device
    
    # ì…ë ¥ ì¤€ë¹„
    enc = processor.apply_transcription_request(
        language=language, 
        audio=audio_path, 
        model_id=model_id, 
        return_tensors="pt"
    )
    
    input_ids = enc["input_ids"].to(device)
    input_features = enc["input_features"].to(device)
    
    # ìƒì„±
    outputs = model.generate(
        input_ids=input_ids,
        input_features=input_features,
        max_new_tokens=max_new_tokens,
        do_sample=False,
        temperature=0.0,
        eos_token_id=processor.tokenizer.eos_token_id,
        pad_token_id=processor.tokenizer.pad_token_id,
        num_beams=1,
        use_cache=False,
    )
    
    # ë””ì½”ë”©
    generated_ids = outputs[0, input_ids.shape[1]:]
    transcription = processor.tokenizer.decode(generated_ids, skip_special_tokens=True)
    
    return transcription

def main():
    parser = argparse.ArgumentParser(description="Voxtral ASR Inference")
    parser.add_argument("--base_model", type=str, default="mistralai/Voxtral-Mini-3B-2507", help="Base model ID")
    parser.add_argument("--adapter_path", type=str, required=True, help="Path to trained adapter")
    parser.add_argument("--audio_path", type=str, help="Path to single audio file")
    parser.add_argument("--test_jsonl", type=str, help="Path to test JSONL file")
    parser.add_argument("--output_path", type=str, help="Path to save results")
    parser.add_argument("--language", type=str, default="ko", help="Language code")
    parser.add_argument("--max_new_tokens", type=int, default=256, help="Max new tokens for generation")
    parser.add_argument("--device", type=str, default="cuda", help="Device to use")
    
    args = parser.parse_args()
    
    print(f"ğŸ”¥ Voxtral ASR Inference ì‹œì‘")
    print(f"Base Model: {args.base_model}")
    print(f"Adapter Path: {args.adapter_path}")
    print(f"Device: {args.device}")
    
    # ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ
    print("ğŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘...")
    processor = AutoProcessor.from_pretrained(args.base_model)
    
    base_model = VoxtralForConditionalGeneration.from_pretrained(
        args.base_model,
        torch_dtype=torch.bfloat16,
        device_map=args.device,
        low_cpu_mem_usage=True
    )
    
    # LoRA ì–´ëŒ‘í„° ë¡œë“œ
    model = PeftModel.from_pretrained(base_model, args.adapter_path)
    model.eval()
    
    print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    # ë‹¨ì¼ íŒŒì¼ ì¶”ë¡ 
    if args.audio_path:
        print(f"\nğŸµ ë‹¨ì¼ íŒŒì¼ ì¶”ë¡ : {args.audio_path}")
        
        if not os.path.exists(args.audio_path):
            print(f"âŒ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.audio_path}")
            return
        
        transcription = inference_single(
            model=model,
            processor=processor,
            audio_path=args.audio_path,
            model_id=args.base_model,
            language=args.language,
            max_new_tokens=args.max_new_tokens
        )
        
        print(f"ğŸ“ ê²°ê³¼: {transcription}")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í‰ê°€
    if args.test_jsonl:
        print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í‰ê°€: {args.test_jsonl}")
        
        if not os.path.exists(args.test_jsonl):
            print(f"âŒ í…ŒìŠ¤íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.test_jsonl}")
            return
        
        results = []
        total_cer = 0.0
        total_wer = 0.0
        count = 0
        
        with open(args.test_jsonl, "r", encoding="utf-8") as f:
            for line_num, line in enumerate(f):
                try:
                    data = json.loads(line.strip())
                    audio_path = data.get("audio") or data.get("audio_path") or ""
                    reference = data.get("text") or data.get("transcript") or ""
                    
                    if not audio_path or not reference:
                        continue
                    
                    if not os.path.exists(audio_path):
                        print(f"âš ï¸  íŒŒì¼ ì—†ìŒ: {audio_path}")
                        continue
                    
                    # ì¶”ë¡  ìˆ˜í–‰
                    hypothesis = inference_single(
                        model=model,
                        processor=processor,
                        audio_path=audio_path,
                        model_id=args.base_model,
                        language=args.language,
                        max_new_tokens=args.max_new_tokens
                    )
                    
                    # ë©”íŠ¸ë¦­ ê³„ì‚°
                    cer_score = cer(reference, hypothesis)
                    wer_score = wer(reference, hypothesis)
                    
                    total_cer += cer_score
                    total_wer += wer_score
                    count += 1
                    
                    result = {
                        "audio_path": audio_path,
                        "reference": reference,
                        "hypothesis": hypothesis,
                        "cer": cer_score,
                        "wer": wer_score
                    }
                    results.append(result)
                    
                    if count % 10 == 0:
                        print(f"ì²˜ë¦¬ ì™„ë£Œ: {count}ê°œ")
                        print(f"í˜„ì¬ í‰ê·  CER: {total_cer/count:.4f}")
                        print(f"í˜„ì¬ í‰ê·  WER: {total_wer/count:.4f}")
                    
                except Exception as e:
                    print(f"âš ï¸  ë¼ì¸ {line_num} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
        
        if count > 0:
            avg_cer = total_cer / count
            avg_wer = total_wer / count
            
            print(f"\nğŸ“ˆ ìµœì¢… ê²°ê³¼:")
            print(f"ì´ ì²˜ë¦¬ëœ ìƒ˜í”Œ: {count}ê°œ")
            print(f"í‰ê·  CER: {avg_cer:.4f}")
            print(f"í‰ê·  WER: {avg_wer:.4f}")
            
            # ê²°ê³¼ ì €ì¥
            if args.output_path:
                with open(args.output_path, "w", encoding="utf-8") as f:
                    for result in results:
                        f.write(json.dumps(result, ensure_ascii=False) + "\n")
                print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ë¨: {args.output_path}")
        else:
            print("âŒ ì²˜ë¦¬ëœ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
