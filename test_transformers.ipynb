{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a43d5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPO ID                          REPO TYPE SIZE ON DISK NB FILES LAST_ACCESSED LAST_MODIFIED REFS LOCAL PATH                                                                   \n",
      "-------------------------------- --------- ------------ -------- ------------- ------------- ---- ---------------------------------------------------------------------------- \n",
      "meta-llama/Llama-3.2-3B-Instruct model            12.9G       15 3 hours ago   3 hours ago   main /home/jhkim/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct \n",
      "mistralai/Voxtral-Mini-3B-2507   model             9.4G        7 4 minutes ago 2 weeks ago   main /home/jhkim/.cache/huggingface/hub/models--mistralai--Voxtral-Mini-3B-2507   \n",
      "\n",
      "Done in 0.0s. Scanned 2 repo(s) for a total of \u001b[1m\u001b[31m22.2G\u001b[0m.\n",
      "\u001b[90mGot 2 warning(s) while scanning. Use -vvv to print details.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!hf cache scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd30c315-90ca-44df-b926-fec61fb0454d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f21e9da7f1144dff96c800ee69d7382d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': '성병은 성적 접촉을 통해 전파되는 질병이지만, 모든 성병은 치료를 받지 않으면 심각한 합병증을 유발하지 않는 것은 아닙니다.\\n\\n성병은 HIV(인플루エ나)와 같은 유전적 질병을 포함한 다양한类型의 질병이 있습니다. HIV는 성적 접촉을 통해 전파되는 질병으로, 치료를 받지 않으면 AIDS(인플루エ나 에이즈)로 진화할 수 있습니다. AIDS는 심각한 합병증을 유발할 수 있습니다, chẳng hạn như 감염, 감염, 심장 질환, 그리고 심지어 사망.\\n\\n다른 성병과는 다르게, HIV는 치료를 받지 않으면 심각한 합병증을 유발하지 않는 것은 아닙니다. HIV는 치료를 받고 제어할 수 있으며, 치료를 받은 사람들은 일반적으로 AIDS로 진화하지 않습니다.\\n\\n그러나, 성병은 심각한 합병증을 유발할 수 있습니다. 예를 들어, 성병은 감염, 감염, 심장 질환, 그리고 심지어 사망'}\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import os\n",
    "\n",
    "model_id = os.getenv(\"MODEL_PATH\", \"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"당신은 의사 입니다, 환자의 질문에 한국어로 답하세요\"},\n",
    "    {\"role\": \"user\", \"content\": \"성병이 성적 접촉을 통해 전파되는 질병인지, 그리고 치료를 받지 않으면 심각한 합병증을 유발할 수 있는가?\"},\n",
    "]\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b17141-4f60-4b80-aeae-7982d3c1136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "간단한 Voxtral 오디오 추론 코드\n",
    "\n",
    "이 스크립트는 오디오 파일을 입력받아 Voxtral 모델로 추론을 수행합니다.\n",
    "sample.py에서 핵심 기능만 추출하여 단순화한 버전입니다.\n",
    "\n",
    "# audio + text\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"audio\", \"url\": audio_url},\n",
    "            {\"type\": \"audio\", \"path\": audio_path},\n",
    "            {\"type\": \"audio\", \"base64\": audio_base64},\n",
    "            {\"type\": \"text\", \"text\": \"How many audio do you hear?\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "processor = VoxtralProcessor.from_pretrained(\"mistralai/Voxtral-Mini-3B-2507\")\n",
    "inputs = processor.apply_chat_template(conversation)\n",
    "\n",
    "# transcription\n",
    "inputs = processor.apply_transcription_request(language=language, audio=audio, model_id=model_id)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "from transformers import VoxtralForConditionalGeneration, AutoProcessor\n",
    "import librosa\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_id = os.getenv(\"MODEL_PATH\", \"/shared/models/mistralai/Voxtral-Mini-3B-2507\")\n",
    "\n",
    "print(f\"사용 디바이스: {device}\")\n",
    "print(f\"모델 로딩 중: {model_id}\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = VoxtralForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=device\n",
    ")\n",
    "\n",
    "audio_path = \"/shared/home/milab/users/kskim/dataset/138.뉴스_대본_및_앵커_음성_데이터/01-1.정식개방데이터/Validation/01.원천데이터/SPK003/SPK003YTNSO162/SPK003YTNSO162F001.wav\"\n",
    "# audio = librosa.load(audio_path, sr=16000)[0]\n",
    "\n",
    "# inputs = processor.apply_transcription_request(language=\"en\", audio=audio_path, model_id=model_id)\n",
    "\n",
    "# inputs = inputs.to(device, dtype=torch.bfloat16 if device == \"cuda\" else torch.float32)\n",
    "# outputs = model.generate(**inputs, max_new_tokens=500)\n",
    "# result = processor.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# print(result[0])\n",
    "\n",
    "\n",
    "# audio_path = '/home/DB/jhkim/speechllm/adapter_finetuning/sample_audio/SPK009YTNSO123M007.wav'\n",
    "text = 'ㅇㅣㅁ'\n",
    "\n",
    "# audio + text\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"audio\", \"path\": audio_path},\n",
    "            # {\"type\": \"text\", \"text\": text},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "inputs = processor.apply_chat_template(conversation)\n",
    "inputs = inputs.to(device, dtype=torch.bfloat16 if device == \"cuda\" else torch.float32)\n",
    "outputs = model.generate(**inputs, max_new_tokens=500)\n",
    "result = processor.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "print(result[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Voxtral",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
